Model 1: AASIST (Audio Anti-Spoofing using Integrated Spectro-Temporal Learning)
ğŸ§ª Key Technical Innovation:
		â—‹ Uses a graph-based attention network that processes both spectral and temporal dimensions.
		â—‹ Incorporates self-attention + convolutional encoding directly on raw audio.
		â—‹ Learns multi-scale relationships using graph attention, making it robust to different spoof types.

ğŸ“Š Reported Performance Metrics:
		â—‹ EER (Equal Error Rate): ~1.84% on ASVspoof 2019 LA.
		â—‹ Accuracy > 97% in controlled datasets.
		â—‹ Ranked 1st in several spoofing challenges.

ğŸ’¡ Why It's Promising for Us:
		â—‹ Works on raw audio, so minimal preprocessing.
		â—‹ Excellent for real conversation-style detection due to temporal modeling.
		â—‹ Strong defense against AI-generated speech â€” including unseen or advanced fakes.
		â—‹ Used in real-world applications (banks, biometrics, forensics).

âš ï¸ Potential Limitations/Challenges:
		â—‹ Heavy architecture â€” not ideal for real-time on low-power devices.
		â—‹ Needs PyTorch + GPU for optimal performance.
		â—‹ Complex training pipeline and tuning.

Model 2: SpecRNet (Spectrogram-based Residual Network)
	
ğŸ§ª Key Technical Innovation:
		â—‹ Uses residual CNN blocks on log-mel spectrograms to detect inconsistencies in frequency patterns.
		â—‹ Leverages skip connections for deeper feature learning and stability.
		â—‹ Focuses on hierarchical spectral patterns â€” where many fakes leak artifacts.

ğŸ“Š Reported Performance Metrics:
		â—‹ EER: ~2.3% â€“ 4.5%
		â—‹ Achieves high accuracy on spoofing datasets using fewer resources than transformer-based models.

ğŸ’¡ Why It's Promising for Us:
		â—‹ Lightweight but performs well â€” good for student projects or proof-of-concept.
		â—‹ Runs on CPU and basic GPU setups.
		â—‹ Simple to build with Keras or PyTorch.
		â—‹ Well-suited for spectrogram-based analysis of short or clipped speech.
	
âš ï¸ Potential Limitations/Challenges:
		â—‹ Less effective on long-form or noisy audio.
		â—‹ Doesnâ€™t model time dependencies explicitly â€” lacks temporal learning like LSTM or attention.

   Model 3: CNN + LSTM Hybrid
	 
ğŸ§ª Key Technical Innovation:
		â—‹ Combines CNNs for spatial feature extraction from MFCCs or spectrograms + LSTM for sequence modeling.
		â—‹ CNN captures local features, LSTM captures global context across time.

ğŸ“Š Reported Performance Metrics:
		â—‹ EER: ~3â€“6% depending on data quality and tuning.
		â—‹ Can outperform pure CNNs on conversational and longer utterances.

ğŸ’¡ Why It's Promising for Us:
		â—‹ Handles real-time conversations better than CNN alone.
		â—‹ Easy to implement with Keras or PyTorch.
		â—‹ Works well with MFCC features â€” which are fast and lightweight.
		â—‹ A nice balance between performance and complexity.

âš ï¸ Potential Limitations/Challenges:
		â—‹ LSTM layers can slow down inference and increase memory usage.
		â—‹ Needs input sequence alignment (padding, trimming).
		â—‹ Can overfit without proper regularization (dropout, batch norm, etc.)

